// Copyright 2019 The Fuchsia Authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#version 460

//
// SEGMENT TTCK
//

#extension GL_GOOGLE_include_directive : require
#extension GL_ARB_gpu_shader_int64 : require
#extension GL_KHR_shader_subgroup_arithmetic : require
#extension GL_KHR_shader_subgroup_ballot : require
#extension GL_KHR_shader_subgroup_shuffle : require
#extension GL_KHR_shader_subgroup_shuffle_relative : require

//
// NOTE THAT THE SEGMENT TTCK KERNEL IS ENTIRELY DEPENDENT ON BOTH THE
// LAYOUT OF THE TTCK KEY AND THE SLAB GEOMETRY.
//
// IF THE TTCK KEY IS ALTERED THEN THIS KERNEL WILL NEED TO BE UPDATED
//
// NOTE THAT WE ASSUME TTCK KEYS WILL ALWAYS BE 64-BIT
//
// TTCK (32-BIT COMPARE) v1:
//
//  0                                                           63
//  | PAYLOAD/TTSB/TTPB ID | PREFIX | ESCAPE | LAYER |  X  |  Y  |
//  +----------------------+--------+--------+-------+-----+-----+
//  |          30          |    1   |    1   |   18  |  7  |  7  |
//
//
// TTCK (32-BIT COMPARE) v2:
//
//  0                                                           63
//  | PAYLOAD/TTSB/TTPB ID | PREFIX | ESCAPE | LAYER |  X  |  Y  |
//  +----------------------+--------+--------+-------+-----+-----+
//  |          30          |    1   |    1   |   15  |  9  |  8  |
//
//
// TTCK (64-BIT COMPARE) -- achieves 4K x 4K with an 8x16 tile: ( DEFAULT )
//
//  0                                                           63
//  | PAYLOAD/TTSB/TTPB ID | PREFIX | ESCAPE | LAYER |  X  |  Y  |
//  +----------------------+--------+--------+-------+-----+-----+
//  |          27          |    1   |    1   |   18  |  9  |  8  |
//

//
//
//

#include "spn_config.h"
#include "spn_vk_layouts.h"

//
//
//

#include "hs_config.h"
#include "hs_glsl_macros_config.h"

//
// specialization constants
//
// FIXME -- we don't know what subgroup size we're going to get with INTEL
//

layout(local_size_x = HS_SLAB_THREADS) in;

//
// NOTE: THIS DESCRIPTOR MUST BE COMPATIBLE WITH 'SPN_DESC_TTRKS'
//

SPN_VK_GLSL_DECL_KERNEL_SEGMENT_TTCK();

//
// DEBUG
//
#if 1

#extension GL_KHR_shader_subgroup_ballot : require
layout(set = 0, binding = 3, std430) buffer _bp_debug
{
  uint                     bp_debug_count[1];
  layout(align = 256) uint bp_debug[];
};

#endif

  //
  //
  //

#if HS_SLAB_HEIGHT > 32
#define HS_COL_FLAG_TYPE uint64_t
#else
#define HS_COL_FLAG_TYPE uint
#endif

  //
  //
  //

#define HS_KEY_TYPE_MAX HS_KEY_TYPE(-1)

  //
  //
  //

#undef HS_KV_OUT
#define HS_KV_OUT ttcks_keys

#undef HS_KV_OUT_LOAD
#define HS_KV_OUT_LOAD(idx) packUint2x32(HS_KV_OUT[idx])

#undef HS_KV_OUT_STORE
#define HS_KV_OUT_STORE(idx, kv) HS_KV_OUT[idx] = unpackUint2x32(kv)

//
//
//

void
main()
{
  HS_SUBGROUP_PREAMBLE();  // because we don't know the size of Intel subgroups

  HS_SLAB_GLOBAL_IDX();

  const uint gmem_out_idx = gmem_idx;
  const uint linear_idx   = gmem_base + gmem_offset * HS_SLAB_HEIGHT;

  //
  // LOAD ALL THE ROWS
  //
#undef HS_SLAB_ROW
#define HS_SLAB_ROW(row, prev) const HS_KEY_TYPE r##row = HS_SLAB_GLOBAL_LOAD_OUT(prev);

  HS_SLAB_ROWS();

  //
  // DEBUG
  //
#if 0
  {
    uint debug_base = 0;

    if (gl_SubgroupInvocationID == 0)
      debug_base = atomicAdd(bp_debug_count[0], HS_KEY_DWORDS * HS_SLAB_THREADS * HS_SLAB_HEIGHT);

    debug_base = subgroupBroadcast(debug_base, 0);

#undef HS_SLAB_ROW
#define HS_SLAB_ROW(row, prev)                                                                     \
  {                                                                                                \
    const uint base = debug_base + prev * HS_SLAB_THREADS * HS_KEY_DWORDS;                         \
                                                                                                   \
    bp_debug[base + gl_SubgroupInvocationID]                   = unpackUint2x32(r##row)[0];        \
    bp_debug[base + HS_SLAB_THREADS + gl_SubgroupInvocationID] = unpackUint2x32(r##row)[1];        \
  }

  HS_SLAB_ROWS();
}
#endif

//
// LOAD LAST REGISTER FROM COLUMN TO LEFT
//
// shuffle up the last key from the column to the left
//
// note that column 0 is undefined
//
HS_KEY_TYPE r0 = HS_SUBGROUP_SHUFFLE_UP(HS_REG_LAST(r), 1);

const bool is_first_lane = (gl_SubgroupInvocationID == 0);

if (is_first_lane)
  {
    if (gmem_base > 0)
      {
        //
        // If this is the first key in any slab but the first then it
        // broadcast loads the last key in previous slab
        //
        // Note that we only need the high dword but not all compilers
        // are cooperating right now
        //
        // FIXME(allanmac): we only need high dword (uvec2[1])
        //
        r0 = HS_KV_OUT_LOAD(gmem_base - 1);
      }
    else
      {
        //
        // If this is the first lane and first slab then record a diff
        //
        r0 = ~r1;
      }
  }

//
// FIND ALL VALID KEYS IN SLAB
//
HS_COL_FLAG_TYPE valid = 0;

#undef HS_SLAB_ROW
#define HS_SLAB_ROW(row, prev) valid |= (r##row != HS_KEY_TYPE_MAX) ? (1u << prev) : 0;

HS_SLAB_ROWS();

//
// FIND ALL DIFFERENCES IN SLAB
//
// Note that the SPN_YX_NEQ() is probably the same number of
// instructions -- eventually choose one and delete the other.
//
#define SPN_HI_XOR(row, prev) (unpackUint2x32(r##row)[1] ^ unpackUint2x32(r##prev)[1])
#define SPN_YX_XOR(row, prev) (SPN_HI_XOR(row, prev) & SPN_TTCK_HI_MASK_YX)

#if 1  // branchless
#define SPN_YX_NEQ(row, prev) (min(1u, SPN_YX_XOR(row, prev)) << prev)
#else  // select
#define SPN_YX_NEQ(row, prev) ((SPN_YX_XOR(row, prev) != 0) ? 1u << prev : 0)
#endif

HS_COL_FLAG_TYPE diffs = 0;

#undef HS_SLAB_ROW
#define HS_SLAB_ROW(row, prev) diffs |= SPN_YX_NEQ(row, prev);

HS_SLAB_ROWS();

//
// SUM UP THE DIFFERENCES
//
const HS_COL_FLAG_TYPE valid_diffs = valid & diffs;
const uint             count       = bitCount(valid_diffs);
const uint             inc         = subgroupInclusiveAdd(count);
const uint             exc         = inc - count;

//
// RESERVE SPACE IN THE INDICES ARRAY
//
uint next = 0;

// FIXME(allanmac) -- need a symbolic offset
if (gl_SubgroupInvocationID == HS_SLAB_THREADS - 1)
  {
    next = atomicAdd(offsets_count[0], inc);
  }

// distribute base across subgroup
next = subgroupBroadcast(next, HS_SLAB_THREADS - 1) + exc;

//
// STORE THE INDICES
//
// FIXME(allanmac): Note that this is *not* going to result in
// coalesced stores but it probably doesn't matter.  If it is
// determined that this impacts performance then it's
// straightforward to accumulate these keys in local memory and
// write them out in a coalesced fashion.
//
#undef HS_SLAB_ROW
#define HS_SLAB_ROW(row, prev)                                                                     \
  if ((valid_diffs & (1 << prev)) != 0)                                                            \
    {                                                                                              \
      offsets[next++] = linear_idx + prev;                                                         \
    }

HS_SLAB_ROWS();

#if 0
#undef HS_SLAB_ROW
#define HS_SLAB_ROW(row, prev)                                                                     \
  if ((valid_diffs & (1 << prev)) != 0)                                                            \
    {                                                                                              \
      const uint debug_base = atomicAdd(bp_debug_count[0], 1);                                     \
      bp_debug[debug_base]  = linear_idx + prev;                                                   \
    }

  HS_SLAB_ROWS();
#endif

//
// TRANSPOSE THE SLAB AND STORE IT
//
HS_TRANSPOSE_SLAB();
}

//
//
//
