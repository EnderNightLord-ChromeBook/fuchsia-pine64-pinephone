// Copyright 2019 The Fuchsia Authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#version 460

//
//
//

#extension GL_GOOGLE_include_directive        : require
#extension GL_KHR_shader_subgroup_basic       : require
#extension GL_KHR_shader_subgroup_ballot      : require

//
//
//

#include "spn_config.h"
#include "spn_vk_layouts.h"

//
//
//

layout(local_size_x = SPN_KERNEL_PATHS_RECLAIM_WORKGROUP_SIZE) in;

//
// main(buffer uint  bp_atomics[2],
//      buffer uint  bp_ids[],
//      buffer uint  bp_blocks[],
//      buffer uint  host_map[],
//      uniform uint bp_mask,
//      uniform uint path_ids[])
//

SPN_VK_GLSL_DECL_KERNEL_PATHS_RECLAIM();

//
// BLOCK EXPANSION
//

#if   (SPN_KERNEL_PATHS_RECLAIM_EXPAND_SIZE == 1)

#define SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND()       SPN_EXPAND_1()
#define SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND_I_LAST  0

#elif (SPN_KERNEL_PATHS_RECLAIM_EXPAND_SIZE == 2)

#define SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND()       SPN_EXPAND_2()
#define SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND_I_LAST  1

#elif (SPN_KERNEL_PATHS_RECLAIM_EXPAND_SIZE == 4)

#define SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND()       SPN_EXPAND_4()
#define SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND_I_LAST  3
#elif (SPN_KERNEL_PATHS_RECLAIM_EXPAND_SIZE == 8)

#define SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND()       SPN_EXPAND_8()
#define SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND_I_LAST  7

#elif (SPN_KERNEL_PATHS_RECLAIM_EXPAND_SIZE == 16)

#define SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND()       SPN_EXPAND_16()
#define SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND_I_LAST  15

#elif (SPN_KERNEL_PATHS_RECLAIM_EXPAND_SIZE == 32)

#define SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND()       SPN_EXPAND_32()
#define SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND_I_LAST  31

#endif

//
// CONSTANTS
//

#define SPN_KERNEL_PATHS_RECLAIM_SUBGROUPS  (SPN_KERNEL_PATHS_RECLAIM_WORKGROUP_SIZE / SPN_KERNEL_PATHS_RECLAIM_SUBGROUP_SIZE)

//
// RUN-TIME PREDICATES
//

#define SPN_PATHS_RECLAIM_BROADCAST(E,O,I)                              \
  subgroupBroadcast(E,O - I * SPN_KERNEL_PATHS_RECLAIM_SUBGROUP_SIZE)

//
//
//

void main()
{
  for (uint reclaim_idx = gl_SubgroupID; reclaim_idx < SPN_KERNEL_PATHS_RECLAIM_MAX_RECLAIM_IDS; reclaim_idx += SPN_KERNEL_PATHS_RECLAIM_SUBGROUPS)
    {
      // get host path id
      const uint path_h = path_ids[reclaim_idx];

      // if it's an invalid id then we're done
      if (path_h == SPN_TAGGED_BLOCK_ID_INVALID)
        return;

      // get the path header block from the map
      uint node_id = bp_host_map[path_h];

      //
      // load the entire head block into registers and start
      // reclaiming blocks
      //
      const uint h_idx = node_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS + gl_SubgroupInvocationID;

#undef  SPN_EXPAND_X
#define SPN_EXPAND_X(I,N,P,L)                                                           \
      uint h##I = bp_blocks[h_idx + I * SPN_KERNEL_PATHS_RECLAIM_SUBGROUP_SIZE];

      SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND();

      //
      // pick out count.blocks and count.nodes from the header
      //
      uint count_blocks, count_nodes;

#undef  SPN_EXPAND_X
#define SPN_EXPAND_X(I,N,P,L)                                                                                   \
      if (SPN_PATH_HEAD_ELEM_IN_RANGE(SPN_KERNEL_PATHS_RECLAIM_SUBGROUP_SIZE,SPN_PATH_HEAD_OFFSET_BLOCKS,I)) {  \
        count_blocks = SPN_PATHS_RECLAIM_BROADCAST(h##I,SPN_PATH_HEAD_OFFSET_BLOCKS,I);                         \
      }                                                                                                         \
      if (SPN_PATH_HEAD_ELEM_IN_RANGE(SPN_KERNEL_PATHS_RECLAIM_SUBGROUP_SIZE,SPN_PATH_HEAD_OFFSET_NODES,I)) {   \
        count_nodes  = SPN_PATHS_RECLAIM_BROADCAST(h##I,SPN_PATH_HEAD_OFFSET_NODES,I);                          \
      }

      SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND();

      //
      // acquire a span in the block pool ids ring for reclaimed ids
      //
      uint bp_ids_base = 0;

      if (gl_SubgroupInvocationID == 0) {
        bp_ids_base = atomicAdd(bp_atomics[1],count_blocks);
      }

      bp_ids_base = subgroupBroadcastFirst(bp_ids_base);

      //
      // invalidate all header components
      //
#undef  SPN_EXPAND_X
#define SPN_EXPAND_X(I,N,P,L)                                                           \
      if (!SPN_PATH_HEAD_ENTIRELY_HEADER(SPN_KERNEL_PATHS_RECLAIM_SUBGROUP_SIZE,I)) {   \
        if (SPN_PATH_HEAD_PARTIALLY_HEADER(SPN_KERNEL_PATHS_RECLAIM_SUBGROUP_SIZE,I)) { \
          if (SPN_PATH_HEAD_IS_HEADER(SPN_KERNEL_PATHS_RECLAIM_SUBGROUP_SIZE,I)) {      \
            h##I = SPN_TAGGED_BLOCK_ID_INVALID;                                         \
          }                                                                             \
        }                                                                               \
      }

      SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND();

      //
      // shift away all tags
      //
#undef  SPN_EXPAND_X
#define SPN_EXPAND_X(I,N,P,L)                                                           \
      if (!SPN_PATH_HEAD_ENTIRELY_HEADER(SPN_KERNEL_PATHS_RECLAIM_SUBGROUP_SIZE,I)) {   \
        h##I = h##I >> SPN_TAGGED_BLOCK_ID_BITS_TAG;                                    \
      }

      SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND();

      //
      // swap the current node id with the "next" id
      //
      if (gl_SubgroupInvocationID == SPN_KERNEL_PATHS_RECLAIM_SUBGROUP_SIZE - 1)
        {
          const uint node_next = SPN_GLSL_CONCAT(h,SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND_I_LAST);

          SPN_GLSL_CONCAT(h,SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND_I_LAST) = node_id;

          node_id = node_next;
        }

      //
      // find ring index of all blocks and store -- FIXME -- NOT COALESCED
      //
#undef  SPN_EXPAND_X
#define SPN_EXPAND_X(I,N,P,L)                                                                   \
      {                                                                                         \
        const bool  is_block     = (h##I & SPN_BLOCK_POOL_SUBBLOCKS_PER_BLOCK_MASK) == 0;       \
        const uvec4 block_ballot = subgroupBallot(is_block);                                    \
        const uint  block_idx    = subgroupBallotExclusiveBitCount(block_ballot);               \
                                                                                                \
        if (is_block)                                                                           \
          bp_ids[(bp_ids_base + block_idx) & bp_mask] = h##I;                                   \
                                                                                                \
        bp_ids_base += subgroupBallotBitCount(block_ballot);                                    \
      }

      SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND();

      //
      // process next node
      //
      while (count_nodes-- > 0)
        {
          // id of next block is in last lane
          node_id = subgroupBroadcast(node_id,SPN_KERNEL_PATHS_RECLAIM_SUBGROUP_SIZE-1);

          //
          // load entire node
          //
          const uint n_idx = node_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS + gl_SubgroupInvocationID;

#undef  SPN_EXPAND_X
#define SPN_EXPAND_X(I,N,P,L)                                                           \
          uint n##I = bp_blocks[n_idx + I * SPN_KERNEL_PATHS_RECLAIM_SUBGROUP_SIZE];

          SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND();

          //
          // shift away all tags
          //
#undef  SPN_EXPAND_X
#define SPN_EXPAND_X(I,N,P,L)                           \
          n##I = n##I >> SPN_TAGGED_BLOCK_ID_BITS_TAG;

          SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND();

          //
          // swap the current node id with the "next" id
          //
          if (gl_SubgroupInvocationID == SPN_KERNEL_PATHS_RECLAIM_SUBGROUP_SIZE - 1)
            {
              const uint node_next = SPN_GLSL_CONCAT(n,SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND_I_LAST);

              SPN_GLSL_CONCAT(n,SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND_I_LAST) = node_id;

              node_id = node_next;
            }

          //
          // find ring index of all blocks and store -- FIXME -- NOT COALESCED
          //
#undef  SPN_EXPAND_X
#define SPN_EXPAND_X(I,N,P,L)                                                                   \
          {                                                                                     \
            const bool  is_block     = (n##I & SPN_BLOCK_POOL_SUBBLOCKS_PER_BLOCK_MASK) == 0;   \
            const uvec4 block_ballot = subgroupBallot(is_block);                                \
            const uint  block_idx    = subgroupBallotExclusiveBitCount(block_ballot);           \
                                                                                                \
            if (is_block)                                                                       \
              bp_ids[(bp_ids_base + block_idx) & bp_mask] = n##I;                               \
                                                                                                \
            bp_ids_base += subgroupBallotBitCount(block_ballot);                                \
          }

          SPN_KERNEL_PATHS_RECLAIM_BLOCK_EXPAND();
        }
    }
}

//
//
//
