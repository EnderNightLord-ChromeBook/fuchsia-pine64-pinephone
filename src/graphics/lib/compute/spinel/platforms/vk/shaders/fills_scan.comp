// Copyright 2019 The Fuchsia Authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#version 460

//
// KERNEL: FILLS SCAN
//

#extension GL_GOOGLE_include_directive : require
#extension GL_KHR_shader_subgroup_basic : require
#extension GL_KHR_shader_subgroup_ballot : require
#extension GL_KHR_shader_subgroup_arithmetic : require

//
//
//

#include "spn_config.h"
#include "spn_vk_layouts.h"

//
//
//

layout(local_size_x = SPN_KERNEL_FILLS_SCAN_WORKGROUP_SIZE) in;

//
//
//

SPN_VK_GLSL_DECL_KERNEL_FILLS_SCAN();

//
// This kernel is executing a prefix-sum on each path primitive count.
//
// A slab of commands will be loaded and scanned in registers with an
// offset determined by a global atomic add.  The prefix sums are then
// stored to global memory.
//

void
main()
{
  //
  // Every subgroup loads a slab of path header primitive counts.
  //
  // The expansion factor can be tuned (bigger is usually better).
  //
  const uint gid_base = gl_GlobalInvocationID.x & ~(SPN_KERNEL_FILLS_SCAN_SUBGROUP_SIZE - 1);
  const uint cmd_base = gid_base * SPN_KERNEL_FILLS_SCAN_ROWS + gl_SubgroupInvocationID;

  //
  // first convert host path ids to device path ids
  //
#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  uint path_id##I = SPN_TAGGED_BLOCK_ID_INVALID;                                                   \
  uint cmd_idx##I = SPN_UINT_MAX;                                                                  \
  {                                                                                                \
    cmd_idx##I = cmd_base + I * SPN_KERNEL_FILLS_SCAN_SUBGROUP_SIZE;                               \
                                                                                                   \
    if (cmd_idx##I < cmd_span)                                                                     \
      {                                                                                            \
        cmd_idx##I += cmd_head;                                                                    \
                                                                                                   \
        if (cmd_idx##I >= cmd_size)                                                                \
          {                                                                                        \
            cmd_idx##I -= cmd_size;                                                                \
          }                                                                                        \
                                                                                                   \
        const uint path_h = SPN_CMD_FILL_GET_PATH_H(fill_cmds[cmd_idx##I]);                        \
                                                                                                   \
        path_id##I = bp_host_map[path_h];                                                          \
      }                                                                                            \
  }

  SPN_KERNEL_FILLS_SCAN_EXPAND();

  //
  // DEBUG -- DUMP ROWS
  //
#if 0
  {
    uint debug_base = 0;

    if (gl_SubgroupInvocationID == 0) {
      debug_base = atomicAdd(bp_debug_count[0],
                             SPN_KERNEL_FILLS_SCAN_ROWS * SPN_KERNEL_FILLS_SCAN_SUBGROUP_SIZE);
    }

    debug_base = subgroupBroadcast(debug_base, 0);

#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  bp_debug[debug_base + I * SPN_KERNEL_FILLS_SCAN_SUBGROUP_SIZE + gl_SubgroupInvocationID] =       \
    path_id##I;

    SPN_KERNEL_FILLS_SCAN_EXPAND();
  }
#endif

  //
  // update the path fill command's path_h member in place with the
  // device block id so we don't have to pay for the potentially
  // sparse map[] lookup twice here and in the FILLS_EXPAND kernel
  //
#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  {                                                                                                \
    if (path_id##I != SPN_TAGGED_BLOCK_ID_INVALID)                                                 \
      {                                                                                            \
        SPN_CMD_RASTERIZE_SET_NODE_ID(fill_cmds[cmd_idx##I], path_id##I);                          \
      }                                                                                            \
  }

  SPN_KERNEL_FILLS_SCAN_EXPAND();

  //
  // load the path header's path primitive count with a uvec4
  //
#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  uvec4 pp##I = { 0, 0, 0, 0 };                                                                    \
  {                                                                                                \
    const uint prims_idx =                                                                         \
      (path_id##I * SPN_BLOCK_POOL_SUBBLOCK_DWORDS + SPN_PATH_HEAD_OFFSET_PRIMS) / 4;              \
                                                                                                   \
    if (path_id##I != SPN_TAGGED_BLOCK_ID_INVALID)                                                 \
      {                                                                                            \
        pp##I = bp_blocks_uvec4[prims_idx];                                                        \
      }                                                                                            \
  }

  SPN_KERNEL_FILLS_SCAN_EXPAND();

  //
  // DEBUG -- dump all valid packed path primitive counts
  //
#if 0
#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  if (path_id##I != SPN_TAGGED_BLOCK_ID_INVALID)                                                   \
    {                                                                                              \
      const uint debug_base = atomicAdd(bp_debug_count[0], 4);                                     \
                                                                                                   \
      bp_debug[debug_base + 0] = pp##I[0];                                                         \
      bp_debug[debug_base + 1] = pp##I[1];                                                         \
      bp_debug[debug_base + 2] = pp##I[2];                                                         \
      bp_debug[debug_base + 3] = pp##I[3];                                                         \
    }

  SPN_KERNEL_FILLS_SCAN_EXPAND();
#endif

  //
  // Incrementally add all the counts -- assumes the slab's packed
  // summations total less than 64m (or 32m for rationals) for each
  // path primitive.
  //
  // This is a reasonable assumption because the block id address
  // space is only 27 bits... but we need to consider enforcing this
  // limit (cheaply!) earlier in the pipeline.
  //
#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  if (I > 0)                                                                                       \
    {                                                                                              \
      uint carry;                                                                                  \
      pp##I[0] = uaddCarry(pp##I[0], pp##P[0], carry);                                             \
      pp##I[1] = uaddCarry(pp##I[1], pp##P[1], carry) + carry;                                     \
      pp##I[2] = uaddCarry(pp##I[2], pp##P[2], carry) + carry;                                     \
      pp##I[3] = uaddCarry(pp##I[3], pp##P[3], carry) + carry;                                     \
    }

  SPN_KERNEL_FILLS_SCAN_EXPAND();

  //
  // Note that most GPU's can perform a subgroup width of atomic ops
  // simultaneously so it's beneficial to have several adjacent lanes
  // performing the same atomic operation.
  //
  // This observation influences the structure of the remaining code.
  //
  // Spread the various prim totals across the first 5 lanes
  //

  //
  // NOTE(allanmac): I'm not very happy with the code below but more
  // concurrency can be added later.
  //
  uint total = 0;

#define SPN_FILLS_SCAN_PP_LAST SPN_GLSL_CONCAT(pp, SPN_KERNEL_FILLS_SCAN_EXPAND_I_LAST)

  //
  // LINES
  //
  const uint last_lines     = SPN_PATH_PRIMS_GET_LINES(SPN_FILLS_SCAN_PP_LAST);
  const uint last_lines_inc = subgroupInclusiveAdd(last_lines);
  const uint total_lines =
    subgroupBroadcast(last_lines_inc, SPN_KERNEL_FILLS_SCAN_SUBGROUP_SIZE - 1);
  const uint last_lines_exc = last_lines_inc - last_lines;

  if (gl_SubgroupInvocationID == SPN_BLOCK_ID_TAG_PATH_LINE)
    total = total_lines;

  //
  // QUADS
  //
  const uint last_quads     = SPN_PATH_PRIMS_GET_QUADS(SPN_FILLS_SCAN_PP_LAST);
  const uint last_quads_inc = subgroupInclusiveAdd(last_quads);
  const uint total_quads =
    subgroupBroadcast(last_quads_inc, SPN_KERNEL_FILLS_SCAN_SUBGROUP_SIZE - 1);
  const uint last_quads_exc = last_quads_inc - last_quads;

  if (gl_SubgroupInvocationID == SPN_BLOCK_ID_TAG_PATH_QUAD)
    total = total_quads;

  //
  // CUBICS
  //
  const uint last_cubics     = SPN_PATH_PRIMS_GET_CUBICS(SPN_FILLS_SCAN_PP_LAST);
  const uint last_cubics_inc = subgroupInclusiveAdd(last_cubics);
  const uint total_cubics =
    subgroupBroadcast(last_cubics_inc, SPN_KERNEL_FILLS_SCAN_SUBGROUP_SIZE - 1);
  const uint last_cubics_exc = last_cubics_inc - last_cubics;

  if (gl_SubgroupInvocationID == SPN_BLOCK_ID_TAG_PATH_CUBIC)
    total = total_cubics;

  //
  // RAT_QUADS
  //
  const uint last_rat_quads     = SPN_PATH_PRIMS_GET_RAT_QUADS(SPN_FILLS_SCAN_PP_LAST);
  const uint last_rat_quads_inc = subgroupInclusiveAdd(last_rat_quads);
  const uint total_rat_quads =
    subgroupBroadcast(last_rat_quads_inc, SPN_KERNEL_FILLS_SCAN_SUBGROUP_SIZE - 1);
  const uint last_rat_quads_exc = last_rat_quads_inc - last_rat_quads;

  if (gl_SubgroupInvocationID == SPN_BLOCK_ID_TAG_PATH_RAT_QUAD)
    total = total_rat_quads;

  //
  // RAT_CUBICS
  //
  const uint last_rat_cubics     = SPN_PATH_PRIMS_GET_RAT_CUBICS(SPN_FILLS_SCAN_PP_LAST);
  const uint last_rat_cubics_inc = subgroupInclusiveAdd(last_rat_cubics);
  const uint total_rat_cubics =
    subgroupBroadcast(last_rat_cubics_inc, SPN_KERNEL_FILLS_SCAN_SUBGROUP_SIZE - 1);
  const uint last_rat_cubics_exc = last_rat_cubics_inc - last_rat_cubics;

  if (gl_SubgroupInvocationID == SPN_BLOCK_ID_TAG_PATH_RAT_CUBIC)
    total = total_rat_cubics;

  //
  // add to the prims.count
  //
  uint base = 0;

  if (gl_SubgroupInvocationID < SPN_BLOCK_ID_TAG_PATH_COUNT)
    base = atomicAdd(fill_scan_counts[gl_SubgroupInvocationID], total);

    //
    // DEBUG -- dump the total and base for each primitive type
    //
#if 0
  {
    uint debug_base = 0;

    if (gl_SubgroupInvocationID == 0)
      debug_base = atomicAdd(bp_debug_count[0], SPN_KERNEL_FILLS_SCAN_SUBGROUP_SIZE * 2);

    debug_base = subgroupBroadcast(debug_base,0);

    bp_debug[debug_base + 0 * SPN_KERNEL_FILLS_SCAN_SUBGROUP_SIZE + gl_SubgroupInvocationID] = base;
    bp_debug[debug_base + 1 * SPN_KERNEL_FILLS_SCAN_SUBGROUP_SIZE + gl_SubgroupInvocationID] = total;
  }
#endif

  //
  // distribute the bases
  //
  const uint base_lines  = subgroupBroadcast(base, SPN_BLOCK_ID_TAG_PATH_LINE) + last_lines_exc;
  const uint base_quads  = subgroupBroadcast(base, SPN_BLOCK_ID_TAG_PATH_QUAD) + last_quads_exc;
  const uint base_cubics = subgroupBroadcast(base, SPN_BLOCK_ID_TAG_PATH_CUBIC) + last_cubics_exc;
  const uint base_rat_quads =
    subgroupBroadcast(base, SPN_BLOCK_ID_TAG_PATH_RAT_QUAD) + last_rat_quads_exc;
  const uint base_rat_cubics =
    subgroupBroadcast(base, SPN_BLOCK_ID_TAG_PATH_RAT_CUBIC) + last_rat_cubics_exc;

  //
  // this is the base for each column in the slab
  //
  const uvec4 pp_base =
    SPN_PATH_PRIMS_INIT(base_lines, base_quads, base_cubics, base_rat_quads, base_rat_cubics);

  //
  // DEBUG -- dump the packed base for the first lane
  //
#if 0
  if (gl_SubgroupInvocationID == 0)
    {
      uint debug_base = atomicAdd(bp_debug_count[0], 4);

      bp_debug[debug_base + 0] = pp_base[0];
      bp_debug[debug_base + 1] = pp_base[1];
      bp_debug[debug_base + 2] = pp_base[2];
      bp_debug[debug_base + 3] = pp_base[3];
    }
#endif

  //
  // write out all the offsets for each commands
  //
#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  {                                                                                                \
    if (cmd_idx##I != SPN_UINT_MAX)                                                                \
      {                                                                                            \
        if (I == 0)                                                                                \
          {                                                                                        \
            fill_scan_prefix[cmd_idx##I] = pp_base;                                                \
          }                                                                                        \
        else                                                                                       \
          {                                                                                        \
            uvec4 ppi##I;                                                                          \
            uint  carry;                                                                           \
                                                                                                   \
            ppi##I[0] = uaddCarry(pp_base[0], pp##P[0], carry);                                    \
            ppi##I[1] = uaddCarry(pp_base[1], pp##P[1], carry) + carry;                            \
            ppi##I[2] = uaddCarry(pp_base[2], pp##P[2], carry) + carry;                            \
            ppi##I[3] = uaddCarry(pp_base[3], pp##P[3], carry) + carry;                            \
                                                                                                   \
            fill_scan_prefix[cmd_idx##I] = ppi##I;                                                 \
          }                                                                                        \
      }                                                                                            \
  }

  SPN_KERNEL_FILLS_SCAN_EXPAND();

  //
  //
  //
}

//
//
//
