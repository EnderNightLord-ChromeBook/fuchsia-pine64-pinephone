// Copyright 2019 The Fuchsia Authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#version 460

//
// SEGMENT TTRK
//
// FIXME - The OpenCL and CUDA versions of this kernel are far more
// sophisticated but let's see how this performs.  Optimize this later
// using the CUDA & OpenCL techniques.
//

#extension GL_GOOGLE_include_directive : require
#extension GL_ARB_gpu_shader_int64 : require
#extension GL_KHR_shader_subgroup_arithmetic : require
#extension GL_KHR_shader_subgroup_ballot : require
#extension GL_KHR_shader_subgroup_shuffle : require
#extension GL_KHR_shader_subgroup_shuffle_relative : require

//
// NOTE THAT THE SEGMENT TTRK KERNEL IS ENTIRELY DEPENDENT ON THE
// LAYOUT OF THE TTRK KEY.
//
// IF THE TTRK KEY IS ALTERED THEN THIS KERNEL WILL NEED TO BE UPDATED
//
// TTRK (64-bit COMPARE)
//
//  0                                         63
//  | TTSB ID |   X  |   Y  | RASTER COHORT ID |
//  +---------+------+------+------------------+
//  |    27   |  12  |  12  |        13        |
//
//
// TTRK (32-BIT COMPARE)
//
//  0                                               63
//  | TTSB ID | N/A |   X  |   Y  | RASTER COHORT ID |
//  +---------+-----+------+------+------------------+
//  |    27   |  5  |  12  |  12  |        8         |
//

//
//
//

#include "spn_config.h"
#include "spn_vk_layouts.h"

//
//
//

#include "hs_config.h"
#include "hs_glsl_macros_config.h"

//
// overridable specialization constants
//

layout(local_size_x = HS_SLAB_THREADS) in;

//
// NOTE: THIS DESCRIPTOR IS COMPATIBLE WITH DS_RASTERIZE_POST
//

SPN_VK_GLSL_DECL_KERNEL_SEGMENT_TTRK();

//
// DEBUG
//
#if 1

#extension GL_KHR_shader_subgroup_ballot : require
layout(set = 0, binding = 3, std430) buffer _bp_debug
{
  uint                     bp_debug_count[1];
  layout(align = 256) uint bp_debug[];
};

#endif

//
// clang-format off
//

#undef  HS_KV_OUT
#define HS_KV_OUT                ttrks_keys

#undef  HS_KV_OUT_LOAD
#define HS_KV_OUT_LOAD(idx)      packUint2x32(HS_KV_OUT[idx])

#undef  HS_KV_OUT_STORE
#define HS_KV_OUT_STORE(idx, kv) HS_KV_OUT[idx] = unpackUint2x32(kv)

//
// clang-format on
//

void
main()
{
  //
  // This is because we didn't know the size of Intel subgroups. The
  // new "subgroup control" extension finally allows us to remove this
  // macro.
  //
  HS_SUBGROUP_PREAMBLE();

  //
  // Unlike the external HotSort shaders, this shader is compiled
  // against the Spinel layout structures and therefore doesn't
  // require use of an opaque "offset" to properly address the sorted
  // keys.
  //
  HS_SLAB_GLOBAL_IDX();

  const uint gmem_out_idx = gmem_idx;
  const uint linear_idx   = gmem_base + gmem_offset * HS_SLAB_HEIGHT;

  //
  // LOAD ALL THE ROWS
  //
#undef HS_SLAB_ROW
#define HS_SLAB_ROW(row, prev) HS_KEY_TYPE r##row = HS_SLAB_GLOBAL_LOAD_OUT(prev);

  HS_SLAB_ROWS();

  //
  // DEBUG
  //
#if 0
  {
    uint debug_base = 0;

    if (gl_SubgroupInvocationID == 0)
      debug_base = atomicAdd(bp_debug_count[0], HS_SLAB_KEYS * HS_KEY_DWORDS);

    debug_base = subgroupBroadcast(debug_base, 0);

#undef HS_SLAB_ROW
#define HS_SLAB_ROW(row, prev)                                                                     \
  {                                                                                                \
    const uint base =                                                                              \
      debug_base + prev * HS_SLAB_THREADS * HS_KEY_DWORDS + gl_SubgroupInvocationID;               \
                                                                                                   \
    const uvec2 r_row = unpackUint2x32(r##row);                                                    \
                                                                                                   \
    bp_debug[base]                   = r_row[0];                                                   \
    bp_debug[base + HS_SLAB_THREADS] = r_row[1];                                                   \
  }

    HS_SLAB_ROWS();
  }
#endif

  //
  // LOAD LAST REGISTER FROM COLUMN TO LEFT
  //
  // shuffle up the last key from the column to the left
  //
  // note that column 0 is undefined
  //
  HS_KEY_TYPE r0 = HS_SUBGROUP_SHUFFLE_UP(HS_REG_LAST(r), 1);

  const bool is_first_lane = (gl_SubgroupInvocationID == 0);

  if (is_first_lane)
    {
      if (gmem_base > 0)
        {
          //
          // If this is the first key in any slab but the first then
          // broadcast load the last key in previous slab from gmem.
          //
          // FIXME(allanmac): note that we only need the high dword --
          // fix this once we harmonize on uvec2 in this shader.
          //
          r0 = HS_KV_OUT_LOAD(gmem_base - 1);
        }
      else
        {
          //
          // This is the first slab and first lane so we want to force
          // recording of a diff.
          //
          r0 = ~r1;
        }
    }

    //
    // FOR ALL VALID KEYS IN SLAB... GATHER META STATS
    //
#define HS_KEY_TO_COHORT_ID(row)                                                                   \
  SPN_BITFIELD_EXTRACT(unpackUint2x32(r##row)[1],                                                  \
                       SPN_TTRK_HI_OFFSET_COHORT,                                                  \
                       SPN_TTRK_HI_BITS_COHORT)

#define SPN_IS_BLOCK(row)                                                                          \
  ((unpackUint2x32(r##row)[0] & SPN_BLOCK_POOL_SUBBLOCKS_PER_BLOCK_MASK) == 0)

    //
    // shift 64-bit TTRK.YX bits downward
    //
    // FIXME(allanmac): make is_pk() work off of a uvec2
    //
#define SPN_XY_XOR(row, prev) uint(((r##row ^ r##prev) >> SPN_TTRK_LO_BITS_TTSB_ID))

    //
    // Detect that only X changed
    //
#define SPN_IS_PK(row, prev)                                                                       \
  ((SPN_XY_XOR(row, prev) - 1) < SPN_GLSL_BITS_TO_MASK(SPN_TTRK_LO_HI_BITS_X))

  uint c      = SPN_UINT_MAX;
  uint c_prev = HS_KEY_TO_COHORT_ID(0);
  uint blocks = 0;
  uint rk     = 0;
  uint pk     = 0;

#undef HS_SLAB_ROW
#define HS_SLAB_ROW(row, prev)                                                                     \
  if (r##row != HS_KEY_VAL_MAX)                                                                    \
    {                                                                                              \
      c = HS_KEY_TO_COHORT_ID(row);                                                                \
                                                                                                   \
      if (c != c_prev)                                                                             \
        {                                                                                          \
          ttrks_meta.alloc[c][SPN_RASTER_COHORT_META_ALLOC_OFFSET_RKOFF] = linear_idx + prev;      \
                                                                                                   \
          if (blocks > 0)                                                                          \
            atomicAdd(ttrks_meta.blocks[c_prev], blocks);                                          \
                                                                                                   \
          if (rk > 0)                                                                              \
            atomicAdd(ttrks_meta.ttrks[c_prev], rk);                                               \
                                                                                                   \
          if (pk > 0)                                                                              \
            atomicAdd(ttrks_meta.ttpks[c_prev], pk);                                               \
                                                                                                   \
          c_prev = c;                                                                              \
          blocks = 0;                                                                              \
          pk     = 0;                                                                              \
          rk     = 0;                                                                              \
        }                                                                                          \
                                                                                                   \
      if (SPN_IS_BLOCK(row))                                                                       \
        blocks += 1;                                                                               \
                                                                                                   \
      if (SPN_IS_PK(row, prev))                                                                    \
        pk += 1;                                                                                   \
                                                                                                   \
      rk += 1;                                                                                     \
    }

  HS_SLAB_ROWS();

  //
  // FIXME(allanmac): there are way too many atomics here!  Reduce them
  // using subgroup operations.
  //

  //
  // IF THERE ARE STRAGGLERS THEN ADD THEM
  //
  if (c != SPN_UINT_MAX)
    {
      if (blocks > 0)
        atomicAdd(ttrks_meta.blocks[c], blocks);

      if (pk > 0)
        atomicAdd(ttrks_meta.ttpks[c], pk);

      // rk will always be > 0
      atomicAdd(ttrks_meta.ttrks[c], rk);
    }

  //
  // TRANSPOSE THE SLAB AND STORE IT
  //
  // FIXME(allanmac): Eventually pass in the buffer name.
  //
  HS_TRANSPOSE_SLAB();
}

//
//
//
