// Copyright 2019 The Fuchsia Authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#version 460

//
//
//

#extension GL_GOOGLE_include_directive : require
#extension GL_EXT_control_flow_attributes : require
#extension GL_KHR_shader_subgroup_basic : require
#extension GL_KHR_shader_subgroup_ballot : require
#extension GL_KHR_shader_subgroup_shuffle : require
#extension GL_KHR_shader_subgroup_shuffle_relative : require
#extension GL_KHR_shader_subgroup_vote : require

//
// PREFIX KERNEL
//

#include "spn_config.h"
#include "spn_vk_layouts.h"

//
// LOCAL DEFINITIONS
//
// clang-format off
//

#define SPN_KERNEL_RASTERS_PREFIX_SUBGROUPS      (SPN_KERNEL_RASTERS_PREFIX_WORKGROUP_SIZE / SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE)
#define SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_MASK  (SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE - 1)
#define SPN_KERNEL_RASTERS_PREFIX_TILES          (SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE / SPN_TILE_HEIGHT)

#define SPN_RP_IS_NEW_Y_MASK                     0x80000000

//
// clang-format on
//

//
//
//

layout(local_size_x = SPN_KERNEL_RASTERS_PREFIX_WORKGROUP_SIZE) in;

//
//
//

SPN_VK_GLSL_DECL_KERNEL_RASTERS_PREFIX();

//
// Size the smem buffer
//
// This shader only requires a subgroup of shared memory.
//

struct spn_rasters_prefix_smem
{
  int buf[SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE];
};

//
//
//

#if (SPN_KERNEL_RASTERS_PREFIX_SUBGROUPS == 1)

shared spn_rasters_prefix_smem smem;

#define SPN_SMEM() smem

#else

shared spn_rasters_prefix_smem smem[SPN_KERNEL_RASTERS_PREFIX_SUBGROUPS];

#define SPN_SMEM() smem[gl_SubgroupID]

#endif

//
//
//

uvec2
spn_ttrk_to_ttsk(const uvec2 ttrk, const uint yx)
{
  //
  // Convert TTRK to TTSK with a span of -1:
  //
  //  0                            63
  //  | TTSB ID |   SPAN  |  X |  Y |
  //  +---------+---------+----+----+
  //  |    27   | 13 [-1] | 12 | 12 |
  //
  return uvec2(SPN_BITFIELD_INSERT(ttrk[0], -1, SPN_TTXK_LO_OFFSET_SPAN, SPN_TTXK_LO_BITS_SPAN),
               SPN_BITFIELD_INSERT(-1, yx, SPN_TTXK_HI_OFFSET_YX, SPN_TTXK_HI_BITS_YX));
}

//
//
//

int
spn_tts_get_dy(const uint tts)
{
  //
  // The tts.dy bitfield is either [-32,-1] or [0,31].
  //
  // After extracting the bitfield, the range must be adjusted:
  //
  //   if (dy >= 0) then ++dy
  //
  // The branchless equivalent subtracts the twiddle shift (~tts>>31)
  // which maps:
  //
  //   [  0,31] -> [  1,32]
  //   [-32,-1] -> [-32,-1]
  //
  // FIXME(allanmac): evaluate performance of branchless since it was
  // implemented this way in CUDA and OpenCL.
  //
  int dy = SPN_TTS_GET_DY(tts);

  if (dy >= 0)
    ++dy;

  return dy;
}

//
//
//

void
spn_ttpb_zero()
{
  SPN_SMEM().buf[gl_SubgroupInvocationID] = 0;
}

void
spn_ttpb_scatter(const uint tts, const uint tile_idx)
{
  if (tts != SPN_TTS_INVALID)
    {
      const int dy = spn_tts_get_dy(tts);

      if (dy != 0)
        {
#if SPN_KERNEL_RASTERS_PREFIX_TILES == 1
          const uint tile_row = SPN_TTS_GET_TY_PIXEL(tts);
#else
          const uint ty       = SPN_TTS_GET_TY_PIXEL(tts);
          const uint tile_row = tile_idx * SPN_TILE_HEIGHT + ty;
#endif

          atomicAdd(SPN_SMEM().buf[tile_row], dy);

#if 0
          {
            const uint debug_base = atomicAdd(bp_debug_count[0], 1);

            bp_debug[debug_base] = dy;
          }
#endif
        }
    }
}

void
spn_ttpb_gather(const uint ttpb_id)
{
  int ttp = SPN_SMEM().buf[gl_SubgroupInvocationID];

#if (SPN_KERNEL_RASTERS_PREFIX_TILES >= 2)

  ttp += subgroupShuffleDown(ttp, SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE / 2);

#endif
#if (SPN_KERNEL_RASTERS_PREFIX_TILES >= 4)

  ttp += subgroupShuffleDown(ttp, SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE / 4);

#endif

#if (SPN_KERNEL_RASTERS_PREFIX_TILES >= 8)
#error "Too many tiles!"
#endif

  //
  // store to TTPB
  //
  if (gl_SubgroupInvocationID < SPN_TILE_HEIGHT)
    {
      const uint ttpb_base = ttpb_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS;

      bp_blocks[ttpb_base + gl_SubgroupInvocationID] = ttp;

      //
      // DEBUG
      //
#if 0
      {
        uint debug_base = 0;

        if (gl_SubgroupInvocationID == 0)
          debug_base = atomicAdd(bp_debug_count[0], SPN_TILE_HEIGHT);

        debug_base = subgroupBroadcast(debug_base, 0) + gl_SubgroupInvocationID;

        bp_debug[debug_base] = ttp;
      }
#endif
    }
}

//
// Construct and initialize a raster object with TTSK keys, TTPK keys
// and associated TTPB blocks.
//

void
main()
{
  //
  // The raster layout is strided because it benefits raster reclamation:
  //
  //   union {
  //     u32   dwords[block_size];
  //     struct {
  //       u32 lo[block_size/2];
  //       u32 hi[block_size/2];
  //     };
  //   };
  //
  // This complicates the PREFIX and PLACE shaders.
  //
  // The raster header's .lo dwords:
  //
  //   uint32_t blocks;  // # of blocks -- head+node+skb+pkb
  //   uint32_t nodes;   // # of nodes  -- not including header
  //   uint32_t pkidx;   // absolute block pool qword of ttpk span
  //   uint32_t ttpks;   // # of ttpks
  //   uint32_t ttsks;   // # of ttsks
  //   uint32_t pknode;  // temporary: relative node of first pk
  //
  // The layout of allocated blocks is as follows:
  //
  //   ... | HEAD(1) | NODES(0+) | TTPB(0+) | ...
  //
  // Unlike previous Spinel implementations, TTPK keys immediately
  // follow TTSK keys.  The starting index of the TTPK keys in the
  // head or nodes is recorded in the raster header.
  //
  // Note that this new implementation accrues costs from redundant
  // loads from bp_ids[] as well as the ttrks_keys[] extents.  On some
  // GPUs these could be marked as uniform buffers.
  //
  // NOTE:
  //
  // There are several separable phases in this shader.  At some point
  // it might be beneficial to split them into concurrent shaders that
  // execute before the final prefix operation.
  //
  // NOTE:
  //
  // There is a subtle assumption here that the number of qwords in a
  // block is greater than or equal to the subgroup size of the target
  // device.  For example, this means that a block must be at least 64
  // qwords (128 dwords) on an AMD GCN3.  This is reasonable.
  //

  //
  // What is the cohort id for this subgroup?
  //
#if (SPN_KERNEL_RASTERS_PREFIX_SUBGROUPS == 1)
  SPN_SUBGROUP_UNIFORM
  const uint cid = gl_WorkGroupID.x;
#else
  SPN_SUBGROUP_UNIFORM
  const uint cid = gl_WorkGroupID.x * SPN_KERNEL_RASTERS_PREFIX_SUBGROUPS + gl_SubgroupID;

  if (cid >= cohort_count)
    return;  // empty subgroup
#endif

  //
  // Get the offset and reads for this cohort id
  //
  SPN_SUBGROUP_UNIFORM const uvec4 meta_alloc = ttrks_meta.alloc[cid];

  //
  // DEBUG
  //
#if 0
  {
    uint debug_base = 0;

    if (gl_SubgroupInvocationID == 0)
      debug_base = atomicAdd(bp_debug_count[0], SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE * 3);

    debug_base = subgroupBroadcast(debug_base, 0) + gl_SubgroupInvocationID;

    bp_debug[debug_base + SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE * 0] = meta_alloc[0];
    bp_debug[debug_base + SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE * 1] = meta_alloc[1];
    bp_debug[debug_base + SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE * 2] = meta_alloc[2];
  }
#endif

  //
  // Blindly read a subgroup of ids
  //
  uint curr_idx = meta_alloc[SPN_RASTER_COHORT_META_ALLOC_OFFSET_READS] + gl_SubgroupInvocationID;
  uint curr_ids = bp_ids[curr_idx & bp_mask];

  //
  // Get the first block id
  //
  uint ttsk_id = subgroupBroadcast(curr_ids, 0);

  //
  // Assumes header qwords fits in subgroup
  //
  // FIXME(allanmac): need to support (.subgroupSize = 4)
  //
  const uint head_base = ttsk_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS;
  uint       header_lo = SPN_UINT_MAX;

  if (gl_SubgroupInvocationID < SPN_RASTER_HEAD_QWORDS)
    {
      header_lo = bp_blocks[head_base + gl_SubgroupInvocationID];
    }

  //
  // How many nodes and keys?
  //
  SPN_SUBGROUP_UNIFORM uint nodes_rem =
    subgroupBroadcast(header_lo, SPN_RASTER_HEAD_LO_OFFSET_NODES);

  SPN_SUBGROUP_UNIFORM uint ttpks_rem =
    subgroupBroadcast(header_lo, SPN_RASTER_HEAD_LO_OFFSET_TTPKS);

  SPN_SUBGROUP_UNIFORM uint ttsks_rem =
    subgroupBroadcast(header_lo, SPN_RASTER_HEAD_LO_OFFSET_TTSKS);

  //
  // DEBUG
  //
#if 0
  {
    uint debug_base = 0;

    if (gl_SubgroupInvocationID == 0)
      debug_base = atomicAdd(bp_debug_count[0], SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE * 1);

    debug_base = subgroupBroadcast(debug_base, 0) + gl_SubgroupInvocationID;

    bp_debug[debug_base + SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE * 0] = header_lo;
  }
#endif

  //
  // Blindly finalize the final node
  //
  SPN_SUBGROUP_UNIFORM const uint ttxks_count = SPN_RASTER_HEAD_QWORDS + ttsks_rem + ttpks_rem;
  SPN_SUBGROUP_UNIFORM const uint ttxks_base  = nodes_rem * (SPN_BLOCK_POOL_BLOCK_QWORDS - 1);
  SPN_SUBGROUP_UNIFORM const uint final_reads =
    meta_alloc[SPN_RASTER_COHORT_META_ALLOC_OFFSET_READS] + nodes_rem;

  SPN_SUBGROUP_UNIFORM const uint final_id   = bp_ids[final_reads & bp_mask];
  SPN_SUBGROUP_UNIFORM const uint final_base = final_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS;

  uint final_off = ttxks_count - ttxks_base + gl_SubgroupInvocationID;

  for (; final_off < SPN_BLOCK_POOL_BLOCK_QWORDS;
       final_off += SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE)
    {
      const uint final_idx = final_base + final_off;

      bp_blocks[final_idx]                               = SPN_TTXK_INVALID[0];
      bp_blocks[final_idx + SPN_BLOCK_POOL_BLOCK_QWORDS] = SPN_TTXK_INVALID[1];
    }

  //
  // Is this an empty raster?
  //
  if (ttsks_rem == 0)
    return;

  //
  // Blindly link the nodes
  //
  //   +---------------- - -
  //   | 0123 4567 89AB ...
  //   |     /    /    /
  //   | 1234 5678 9ABC ...
  //   +---------------- - -
  //
  if (nodes_rem >= 1)
    {
      while (true)
        {
          uint link_ids = subgroupShuffleDown(curr_ids, 1);
          uint next_ids = SPN_BLOCK_ID_INVALID;

          //
          // DEBUG
          //
#if 0
          {
            uint debug_base = 0;

            if (gl_SubgroupInvocationID == 0)
              debug_base = atomicAdd(bp_debug_count[0], SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE);

            debug_base = subgroupBroadcast(debug_base, 0) + gl_SubgroupInvocationID;

            bp_debug[debug_base] = link_ids;
          }
#endif

          if (nodes_rem >= SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE)
            {
              //
              // NOTE: as noted in other shaders, Intel supports the
              // idiom below with an enhanced shuffle instruction.
              // Hopefully it comes to Vulkan (from OpenCL) via an
              // extension.
              //
              curr_idx += SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE;

              next_ids = bp_ids[curr_idx & bp_mask];

              const uint next_id0 = subgroupBroadcast(next_ids, 0);

              if (gl_SubgroupInvocationID == SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE - 1)
                {
                  link_ids = next_id0;
                }
            }

          if (gl_SubgroupInvocationID < nodes_rem)
            {
              const uint curr_base = curr_ids * SPN_BLOCK_POOL_SUBBLOCK_DWORDS;

              bp_blocks[curr_base + SPN_BLOCK_POOL_BLOCK_QWORDS - 1] = link_ids;
              bp_blocks[curr_base + SPN_BLOCK_POOL_BLOCK_DWORDS - 1] = 0;
              // the upper dword must have a span of zero
            }

          // done?
          if (nodes_rem <= SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE)
            break;

          // we linked a subgroup of nodes
          nodes_rem -= SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE;

          // swap and continue
          curr_ids = next_ids;
        }
    }

  //
  // Otherwise:
  //
  // 1. For all R ttrk keys:
  //
  //   - load ttrk keys
  //   - convert to ttsk
  //   - store ttsk to destination node
  //   - store from/to indices of ttsks for a ttpk span
  //
  // 2. Finalize the final node
  //
  // 3. subgroupBarrier()
  //
  // 4. For all P ttpk keys:
  //
  //   - scatter-accumulate ttp values
  //   - store to TTPB
  //

  //
  // Prepare to store TTSK keys into the head node
  //
  // The subtle but safe assumption here is that the sum of header and
  // subgroup size is less than two blocks of qwords.
  //
  uint ttrk_idx   = meta_alloc[SPN_RASTER_COHORT_META_ALLOC_OFFSET_RKOFF] + gl_SubgroupInvocationID;
  uint ttsk_off   = SPN_RASTER_HEAD_QWORDS + gl_SubgroupInvocationID;
  uint ttsk_reads = meta_alloc[SPN_RASTER_COHORT_META_ALLOC_OFFSET_READS];

  SPN_SUBGROUP_UNIFORM const uint pk_idx =
    subgroupBroadcast(header_lo, SPN_RASTER_HEAD_LO_OFFSET_PKIDX);

  uint ttpk_reads = meta_alloc[SPN_RASTER_COHORT_META_ALLOC_OFFSET_PKNODE];
  uint ttpk_id    = bp_ids[ttpk_reads & bp_mask];
  uint ttpk_off   = pk_idx & SPN_BLOCK_POOL_BLOCK_DWORDS_MASK;

  // start with impossible out of range value
  uint yx_last = 0x01FFFFFF;

  // running count of yx changes
  SPN_SUBGROUP_UNIFORM uint nz_count = 0;

  while (true)
    {
      // the node boundary requires special treatment
      if (ttsk_off >= SPN_BLOCK_POOL_BLOCK_QWORDS - 1)
        {
          // reset to start of next node
          ttsk_off -= SPN_BLOCK_POOL_BLOCK_QWORDS - 1;

          // load next node id -- FIXME(allanmac): make this faster
          ttsk_id = bp_ids[++ttsk_reads & bp_mask];
        }

#if 0
          {
            uint debug_base = 0;

            if (gl_SubgroupInvocationID == 0)
              debug_base = atomicAdd(bp_debug_count[0], SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE * 2);

            debug_base = subgroupBroadcast(debug_base, 0) + gl_SubgroupInvocationID;

            bp_debug[debug_base + SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE * 0] = ttsk_off;
            bp_debug[debug_base + SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE * 1] = ttsk_id;
          }
#endif

      const bool is_valid = (gl_SubgroupInvocationID < ttsks_rem);
      const uint ttsk_idx = ttsk_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS + ttsk_off;

      uint  yx;
      uvec2 ttsk;

      if (is_valid)
        {
          // load ttrk
          const uvec2 ttrk = ttrks_keys[ttrk_idx];

          // save the yx
          yx = SPN_TTRK_GET_YX(ttrk);

          // convert ttrk to ttsk
          ttsk = spn_ttrk_to_ttsk(ttrk, yx);

          bp_blocks[ttsk_idx]                               = ttsk[0];
          bp_blocks[ttsk_idx + SPN_BLOCK_POOL_BLOCK_QWORDS] = ttsk[1];
        }

      //
      // NOTE: as noted in other shaders, Intel supports the idiom
      // below with an enhanced shuffle instruction.  Hopefully it
      // comes to Vulkan (from OpenCL) via an extension.
      //
      // detect TTPK key
      //
      uint yx_prev = subgroupShuffleUp(yx, 1);

      // lane 0 value is undefined and high lanes might be inactive
      if (gl_SubgroupInvocationID == 0)
        {
          yx_prev = yx_last;
        }

      uint xy_xor = 0;

      if (is_valid)
        {
          xy_xor = yx ^ yx_prev;
        }

      const bool is_nz = (xy_xor != 0);

      // is this either a y change or a yx change?
      if (subgroupAny(is_nz))
        {
          const uvec4 nz_ballot = subgroupBallot(is_nz);
          const uint  nz_exc    = subgroupBallotExclusiveBitCount(nz_ballot);
          const uint  nz_off    = nz_count + nz_exc;
          const uint  nz_idx    = nz_off & SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_MASK;
          const bool  is_ttpk   = uint(xy_xor - 1) < SPN_YX_X_MASK;

          // mark y changes
          if (is_nz)
            {
              const uint new_y_mask = is_ttpk ? 0 : SPN_RP_IS_NEW_Y_MASK;

              SPN_SMEM().buf[nz_idx] = int(ttsk_idx | new_y_mask);
            }

          nz_count += subgroupBallotBitCount(nz_ballot);

          if (subgroupAny(is_ttpk))
            {
              // read-after-write barrier
              subgroupBarrier();

              const uvec4 ttpk_ballot = subgroupBallot(is_ttpk);
              const uint  ttpk_exc    = subgroupBallotExclusiveBitCount(ttpk_ballot);

              if (is_ttpk)
                {
                  ttpk_off += ttpk_exc;

                  if (ttpk_off >= SPN_BLOCK_POOL_BLOCK_QWORDS - 1)
                    {
                      // load next node id -- FIXME(allanmac): make this faster
                      ttpk_id = bp_ids[++ttpk_reads & bp_mask];

                      // reset to start of next node
                      ttpk_off -= SPN_BLOCK_POOL_BLOCK_QWORDS - 1;
                    }

                  //
                  // RECORD FROM/TO TTSK INDICES
                  //
                  const uint nz_idx_prev   = (nz_idx - 1) & SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_MASK;
                  const uint ttsk_idx_prev = uint(SPN_SMEM().buf[nz_idx_prev]);
                  const uint ttpk_idx      = ttpk_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS + ttpk_off;

                  bp_blocks[ttpk_idx]                               = ttsk_idx_prev;
                  bp_blocks[ttpk_idx + SPN_BLOCK_POOL_BLOCK_QWORDS] = int(ttsk_idx);

                  ttpk_off += 1;
                }

              const uint ttpk_lane = subgroupBallotFindMSB(ttpk_ballot);

              ttpk_reads = subgroupShuffle(ttpk_reads, ttpk_lane);
              ttpk_id    = subgroupShuffle(ttpk_id, ttpk_lane);
              ttpk_off   = subgroupShuffle(ttpk_off, ttpk_lane);
            }
        }

      // was this the last subgroup of keys?
      if (ttsks_rem <= SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE)
        break;

      // save last yx in subgroup
      yx_last = subgroupBroadcast(yx, SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE - 1);

      // there are still keys to process...
      ttsks_rem -= SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE;

      // bump the ttrk index
      ttrk_idx += SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE;

      // bump the node offset
      ttsk_off += SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE;
    }

  // anything to do?
  if (ttpks_rem == 0)
    return;

  //
  // flush and go back to work on prefix
  //
  subgroupBarrier();

  //
  // for all quasi-TTPK keys
  //
  //   - update TTPK key with subblock id
  //   - for all TTSK keys in a TTPK key
  //     - accumulate the TTP values
  //

  // ttpb subblocks start here
  SPN_SUBGROUP_UNIFORM const uint ttpb_reads = final_reads + 1;

  // subblock offset
  uint ttpb_off = gl_SubgroupInvocationID;

  //
  // avoid recomputing the subgroup<>tile mapping
  //
#if (SPN_KERNEL_RASTERS_PREFIX_TILES == 1)

#define SPN_TTSB_TILE_IDX 0
#define SPN_TTSB_TILE_IID gl_SubgroupInvocationID

#else  // multiple tiles per subgroup

  const uint spn_ttsb_tile_idx = gl_SubgroupInvocationID / SPN_TILE_HEIGHT;
  const uint spn_ttsb_tile_iid = gl_SubgroupInvocationID & SPN_TILE_HEIGHT_MASK;

#define SPN_TTSB_TILE_IDX spn_ttsb_tile_idx
#define SPN_TTSB_TILE_IID spn_ttsb_tile_iid

#endif

  // directly jump to first ttpk key
  uint ttpk_base = (pk_idx & ~SPN_BLOCK_POOL_BLOCK_DWORDS_MASK);
  ttpk_off       = (pk_idx & SPN_BLOCK_POOL_BLOCK_DWORDS_MASK) + gl_SubgroupInvocationID;

  while (true)
    {
      //
      //           pk_idx[0] ... pk_idx[P-1]
      //              |             |
      //              |             .
      //         sk_from_to[2]      .
      //             / \            .
      //            /   \           .
      //       ttsk0 ... ttskN
      //
      const bool is_pk_valid = (gl_SubgroupInvocationID < ttpks_rem);

      // adjust pk_idx
      if (is_pk_valid)
        {
          if (ttpk_off >= SPN_BLOCK_POOL_BLOCK_QWORDS - 1)
            {
              // FIXME(allanmac): alternatively read this from the bp_ids[]
              const uint block_id = bp_blocks[ttpk_base + SPN_BLOCK_POOL_BLOCK_QWORDS - 1];

              ttpk_base = block_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS;

              ttpk_off -= (SPN_BLOCK_POOL_BLOCK_QWORDS - 1);
            }
        }

      //
      // load subgroup of sk_from_to[2] records
      //
      uvec2 sk_from_quit_idx;
      uint  ttpb_id;

      if (is_pk_valid)
        {
          const uint pk_idx = ttpk_base + ttpk_off;

          // get from/quit dword
          sk_from_quit_idx[0] = bp_blocks[pk_idx];
          sk_from_quit_idx[1] = bp_blocks[pk_idx + SPN_BLOCK_POOL_BLOCK_QWORDS];

          // get the TTSK.HI dword to obtain YX span
          const uint sk_from_idx = sk_from_quit_idx[0] & ~SPN_RP_IS_NEW_Y_MASK;
          const uint sk_from_hi  = bp_blocks[sk_from_idx + SPN_BLOCK_POOL_BLOCK_QWORDS];
          const uint sk_quit_hi  = bp_blocks[sk_from_quit_idx[1] + SPN_BLOCK_POOL_BLOCK_QWORDS];

          //
          // Replace the sk_from_to[2] with a TTPK
          //
          // get subblock id
          const uint ttpb_block    = ttpb_reads + ttpb_off / SPN_BLOCK_POOL_SUBBLOCKS_PER_BLOCK;
          const uint ttpb_subblock = ttpb_off & SPN_BLOCK_POOL_SUBBLOCKS_PER_BLOCK_MASK;

          // load the TTPB id
          ttpb_id = bp_ids[ttpb_block & bp_mask] + ttpb_subblock;

          // update span
          const uint span = SPN_TTXK_HI_GET_YX(sk_quit_hi - sk_from_hi);

          // construct TTPK key
          uvec2 ttpk = { ttpb_id, sk_from_hi + SPN_TTXK_HI_ONE_X };

          SPN_TTXK_SET_SPAN(ttpk, span);

          //
          // DEBUG
          //
#if 0
          {
            const uint debug_base = atomicAdd(bp_debug_count[0], 3);

            bp_debug[debug_base + 0] = sk_from_hi;
            bp_debug[debug_base + 1] = sk_quit_hi;
            bp_debug[debug_base + 2] = span;
          }
#endif

          // write it back
          bp_blocks[pk_idx]                               = ttpk[0];
          bp_blocks[pk_idx + SPN_BLOCK_POOL_BLOCK_QWORDS] = ttpk[1];
        }

      //
      // Accumulate altitudes for TTPB
      //
      const uint pk_valid_count = min(ttpks_rem, SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE);

      //
      // For each sk_from_to[2]:
      //   For TTSK keys in range [lo,hi)
      //     For all tiles in a subgroup
      //       - load TTSB_ID
      //       - load TTS
      //       - convert TTS to TTP
      //       - scatter TTP values
      //     End
      //   End
      //   Reduce tiles of TTP values into a TTPB
      //   Store TTPB to subblock
      //
      for (uint ii = 0; ii < pk_valid_count; ii++)
        {
          //
          // accumulate one TTPB at a time
          //
          SPN_SUBGROUP_UNIFORM const uint sk_from_idx_y = subgroupShuffle(sk_from_quit_idx[0], ii);
          SPN_SUBGROUP_UNIFORM const uint sk_from_idx   = sk_from_idx_y & ~SPN_RP_IS_NEW_Y_MASK;

          // record if this was a new y
          if (sk_from_idx_y != sk_from_idx)
            {
              spn_ttpb_zero();
            }

          // mask it off
          uvec2 sk_from = {

            (sk_from_idx & ~SPN_BLOCK_POOL_BLOCK_DWORDS_MASK),
            (sk_from_idx & SPN_BLOCK_POOL_BLOCK_DWORDS_MASK) + gl_SubgroupInvocationID
          };

          SPN_SUBGROUP_UNIFORM const uint  sk_quit_idx = subgroupShuffle(sk_from_quit_idx[1], ii);
          SPN_SUBGROUP_UNIFORM const uvec2 sk_quit     = {

            (sk_quit_idx & ~SPN_BLOCK_POOL_BLOCK_DWORDS_MASK),
            (sk_quit_idx & SPN_BLOCK_POOL_BLOCK_DWORDS_MASK)
          };

          while (true)
            {
              //
              // Fixup TTSK key indices
              //
              // Note: intermediate blocks are implicitly valid.
              //
              const bool is_same_block = (sk_from[0] == sk_quit[0]);
              bool       is_sk_valid   = true;

              // valid offset?
              if (sk_from[1] < SPN_BLOCK_POOL_BLOCK_QWORDS - 1)
                {
                  // if same block then test against quit offset
                  if (is_same_block)
                    {
                      is_sk_valid = sk_from[1] < sk_quit[1];
                    }
                }
              else  // out of range offset
                {
                  if (is_same_block)  // illegal offset in same block
                    {
                      is_sk_valid = false;
                    }
                  else  // otherwise, update block and offset
                    {
                      const uint block_id = bp_blocks[sk_from[0] + SPN_BLOCK_POOL_BLOCK_QWORDS - 1];

                      sk_from[0] = block_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS;
                      sk_from[1] -= (SPN_BLOCK_POOL_BLOCK_QWORDS - 1);

                      // if same block then test against quit offset
                      if (sk_from[0] == sk_quit[0])
                        {
                          is_sk_valid = sk_from[1] < sk_quit[1];
                        }
                    }
                }

              //
              // Load up to a subgroup of TTSB_IDs
              //
              uint ttsb_ids = SPN_BLOCK_ID_INVALID;

              if (is_sk_valid)
                {
                  const uint ttsk_lo = bp_blocks[sk_from[0] + sk_from[1]];
                  ttsb_ids           = SPN_TTXK_LO_GET_TTXB_ID(ttsk_lo);
                }

              //
              // How many are valid?
              //
              // FIXME(allanmac): avoid using a ballot here
              //
              const uint sk_valid_count = subgroupBallotBitCount(subgroupBallot(is_sk_valid));

              // load SPN_KERNEL_RASTERS_PREFIX_TILES at a time
              for (uint jj = 0; jj < sk_valid_count; jj += SPN_KERNEL_RASTERS_PREFIX_TILES)
                {
                  const uint ttsb_id = subgroupShuffle(ttsb_ids, jj + SPN_TTSB_TILE_IDX);

                  if (ttsb_id != SPN_BLOCK_ID_INVALID)
                    {
                      const uint ttsb_base = ttsb_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS;
                      const uint ttsb_idx  = ttsb_base + SPN_TTSB_TILE_IID;
                      const uint tts       = bp_blocks[ttsb_idx];

                      spn_ttpb_scatter(tts, SPN_TTSB_TILE_IDX);
                    }
                }

              // if not a full subgroup then we know this was the final load
              if (sk_valid_count < SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE)
                break;

              // increment sk_from[1] and go again
              sk_from[1] += SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE;
            }

          // read-after-write
          subgroupBarrier();

          // store accumulated altitudes to TTPB
          spn_ttpb_gather(subgroupShuffle(ttpb_id, ii));
        }

      // done?
      if (ttpks_rem <= SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE)
        break;

      ttpks_rem -= SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE;
      ttpb_off += SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE;
      ttpk_off += SPN_KERNEL_RASTERS_PREFIX_SUBGROUP_SIZE;
    }
}

//
//
//
