// Copyright 2019 The Fuchsia Authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#version 460

//
// KERNEL: FILLS EXPAND
//

#extension GL_GOOGLE_include_directive : require
#extension GL_KHR_shader_subgroup_basic : require
#extension GL_KHR_shader_subgroup_vote : require
#extension GL_KHR_shader_subgroup_ballot : require
#extension GL_KHR_shader_subgroup_arithmetic : require
#extension GL_KHR_shader_subgroup_shuffle : require

//
//
//

#include "spn_config.h"
#include "spn_vk_layouts.h"

//
// Note that we may want to launch a workgroup of multiple subgroups
// but typically will be launching a single subgroup.
//
// REMOVEME(allanmac): once subgroup_size_control is ready:
//
// Intel's variable subgroup size is currently not controllable and
// complicates reasoning about expected code generation.
//
// For Intel, the current strategy for kernels that *must* know the
// subgroup size before codegen is to simply clamp the workgroup size
// to the smallest subgroup size (which is 8) which means there can be
// no multi-subgroup workgroups.
//

layout(local_size_x = SPN_KERNEL_FILLS_EXPAND_WORKGROUP_SIZE) in;

//
//
//

SPN_VK_GLSL_DECL_KERNEL_FILLS_EXPAND();

//
//
//

#if (SPN_BLOCK_POOL_BLOCK_DWORDS / SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE == 1)

#define SPN_FILLS_EXPAND_BLOCK_EXPAND() SPN_EXPAND_1()
#define SPN_FILLS_EXPAND_BLOCK_EXPAND_I_LAST 0

#elif (SPN_BLOCK_POOL_BLOCK_DWORDS / SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE == 2)

#define SPN_FILLS_EXPAND_BLOCK_EXPAND() SPN_EXPAND_2()
#define SPN_FILLS_EXPAND_BLOCK_EXPAND_I_LAST 1

#elif (SPN_BLOCK_POOL_BLOCK_DWORDS / SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE == 4)

#define SPN_FILLS_EXPAND_BLOCK_EXPAND() SPN_EXPAND_4()
#define SPN_FILLS_EXPAND_BLOCK_EXPAND_I_LAST 3

#elif (SPN_BLOCK_POOL_BLOCK_DWORDS / SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE == 8)

#define SPN_FILLS_EXPAND_BLOCK_EXPAND() SPN_EXPAND_8()
#define SPN_FILLS_EXPAND_BLOCK_EXPAND_I_LAST 7

#elif (SPN_BLOCK_POOL_BLOCK_DWORDS / SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE == 16)

#define SPN_FILLS_EXPAND_BLOCK_EXPAND() SPN_EXPAND_16()
#define SPN_FILLS_EXPAND_BLOCK_EXPAND_I_LAST 15

#elif (SPN_BLOCK_POOL_BLOCK_DWORDS / SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE == 32)

#define SPN_FILLS_EXPAND_BLOCK_EXPAND() SPN_EXPAND_32()
#define SPN_FILLS_EXPAND_BLOCK_EXPAND_I_LAST 31

#endif

//
// COMPILE-TIME CONSTANTS
//

#define SPN_KERNEL_FILLS_EXPAND_SUBGROUPS                                                          \
  (SPN_KERNEL_FILLS_EXPAND_WORKGROUP_SIZE / SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE)

//
// RUN-TIME PREDICATES
//

#define SPN_FILLS_EXPAND_BROADCAST(E, O, I)                                                        \
  subgroupBroadcast(E, O - I * SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE)

//
// UTILITY
//

#define SPN_CMD_RASTERIZE_I_TO_NODE_DWORD(I)                                                       \
  (I * SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE + gl_SubgroupInvocationID)

//
//
//

void
main()
{
  //
  // every subgroup/simd loads the same command
  //
#if (SPN_KERNEL_FILLS_EXPAND_SUBGROUPS == 1)
  SPN_SUBGROUP_UNIFORM
  uint sid = gl_WorkGroupID.x;
#else
  SPN_SUBGROUP_UNIFORM
  uint sid = gl_WorkGroupID.x * SPN_KERNEL_FILLS_EXPAND_SUBGROUPS + gl_SubgroupID;

  // if the workgroup is made up of multiple subgroups then exit if
  // the trailing subgroups have no work to do
  if (sid >= cmd_span)
    return;
#endif

  //
  // where are we in the ring?
  //
  sid += cmd_head;

  if (sid >= cmd_size)
    {
      sid -= cmd_size;
    }

  //
  // Each subgroup loads an spn_cmd_fill and the packed prim counts
  // associated with this command.
  //
  // Note that the 'path_h' component was converted to a node_id in
  // order to avoid a sparse lookup in the map[].
  //
  // The cmd is not subgroup uniform because we repeatedly reuse it in
  // each lane.
  //
  uvec4 cmd = fill_cmds[sid];

  //
  // these are subgroup uniform
  //
  SPN_SUBGROUP_UNIFORM uint        node_id          = SPN_CMD_RASTERIZE_GET_NODE_ID(cmd);
  SPN_SUBGROUP_UNIFORM const uvec4 rast_base_packed = fill_scan_prefix[sid];

  //
  // DEBUG
  //
#if 0
  {
    if (gl_SubgroupInvocationID == 0)
      {
        uint debug_base = atomicAdd(bp_debug_count[0], 5);

        bp_debug[debug_base + 0] = rast_base_packed[0];
        bp_debug[debug_base + 1] = rast_base_packed[1];
        bp_debug[debug_base + 2] = rast_base_packed[2];
        bp_debug[debug_base + 3] = rast_base_packed[3];
        bp_debug[debug_base + 4] = sid;
      }
  }
#endif

  //
  // vertically count tags
  //
  // clang-format off
  //
#if SPN_BLOCK_ID_TAG_PATH_COUNT <= 5
  //
  // 32-bits can hold 5 6-bit fields of [0,63]
  //
#define SPN_FILLS_EXPAND_TAG_COUNT_TYPE         uint
#define SPN_FILLS_EXPAND_TAG_COUNT_INIT(n_)     SPN_FILLS_EXPAND_TAG_COUNT_TYPE n_ = 0
#define SPN_FILLS_EXPAND_TAG_COUNT_GET(c_, i_)  SPN_BITFIELD_EXTRACT(c_,int(i_*6),6)
// #define SPN_FILLS_EXPAND_TAG_COUNT_GET(c, i)  ((c >> (i * 6)) & ((1 << 6) - 1))

  //
  // Left shifting >= 32 doesn't saturate to zero. The spec speaks to
  // this but it's still a little vague.
  //
  // FIXME(allanmac): make this a maskable operation by altering the
  // invalid path flag
  //
#define SPN_FILLS_EXPAND_TAG_COUNT_ADD(c_, t_)                    \
  c_ += ((t_ < SPN_BLOCK_ID_TAG_PATH_COUNT ? 0x1 : 0x0) << (t_ * 6))

  //
  // Fill this in if there are ever more than 5 path primitive types.
  //
#else
#error FIXME -- no support for more than 5 tags.
#endif
  //
  // clang-format on
  //

  //
  // load entire head
  //
  const uint h_idx = node_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS + gl_SubgroupInvocationID;

#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  uint h##I = bp_blocks[h_idx + I * SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE];

  SPN_FILLS_EXPAND_BLOCK_EXPAND();

  //
  // DEBUG -- dump the head
  //
#if 0
  {
    uint debug_base = 0;

    if (gl_SubgroupInvocationID == 0)
      debug_base = atomicAdd(bp_debug_count[0], SPN_BLOCK_POOL_BLOCK_DWORDS);

    debug_base = subgroupBroadcast(debug_base, 0);

#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  bp_debug[debug_base + I * SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE + gl_SubgroupInvocationID] = h##I;

    SPN_FILLS_EXPAND_BLOCK_EXPAND();
  }
#endif

  //
  // pick out count.nodes from the header
  //
  uint count_nodes;

#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  if (SPN_PATH_HEAD_ELEM_IN_RANGE(SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE,                           \
                                  SPN_PATH_HEAD_OFFSET_NODES,                                      \
                                  I))                                                              \
    {                                                                                              \
      count_nodes = SPN_FILLS_EXPAND_BROADCAST(h##I, SPN_PATH_HEAD_OFFSET_NODES, I);               \
    }

  SPN_FILLS_EXPAND_BLOCK_EXPAND();

  //
  // invalidate header components
  //
#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  if (!SPN_PATH_HEAD_ENTIRELY_HEADER(SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE, I))                    \
    {                                                                                              \
      if (SPN_PATH_HEAD_PARTIALLY_HEADER(SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE, I))                \
        {                                                                                          \
          if (SPN_PATH_HEAD_IS_HEADER(SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE, I))                   \
            {                                                                                      \
              h##I = SPN_TAGGED_BLOCK_ID_INVALID;                                                  \
            }                                                                                      \
        }                                                                                          \
    }

  SPN_FILLS_EXPAND_BLOCK_EXPAND();

  //
  // DEBUG -- dump the head with its header invalidated
  //
#if 0
  {
    uint debug_base = 0;

    if (gl_SubgroupInvocationID == 0)
      debug_base = atomicAdd(bp_debug_count[0], SPN_BLOCK_POOL_BLOCK_DWORDS);

    debug_base = subgroupBroadcast(debug_base,0);

#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  bp_debug[debug_base + I * SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE + gl_SubgroupInvocationID] = h##I;

    SPN_FILLS_EXPAND_BLOCK_EXPAND();
  }
#endif

  //
  // unpack counts
  //
  uint rast_base = 0;

  //
  // FIXME(allanmac): UNPACK THESE HORIZONTALLY / IN PARALLEL -- this
  // is sequentially unpacking the tag counts.
  //
  // FIXME(allanmac): this is assuming a subgroup size of >= 5 --
  // won't work for quad-sized subgroups (ARM Bifrost v1, iOS, ...)
  // without updating this logic.
  //
  if (gl_SubgroupInvocationID == SPN_BLOCK_ID_TAG_PATH_LINE)
    rast_base = SPN_PATH_PRIMS_GET_LINES(rast_base_packed);

  if (gl_SubgroupInvocationID == SPN_BLOCK_ID_TAG_PATH_QUAD)
    rast_base = SPN_PATH_PRIMS_GET_QUADS(rast_base_packed);

  if (gl_SubgroupInvocationID == SPN_BLOCK_ID_TAG_PATH_CUBIC)
    rast_base = SPN_PATH_PRIMS_GET_CUBICS(rast_base_packed);

  if (gl_SubgroupInvocationID == SPN_BLOCK_ID_TAG_PATH_RAT_QUAD)
    rast_base = SPN_PATH_PRIMS_GET_RAT_QUADS(rast_base_packed);

  if (gl_SubgroupInvocationID == SPN_BLOCK_ID_TAG_PATH_RAT_CUBIC)
    rast_base = SPN_PATH_PRIMS_GET_RAT_CUBICS(rast_base_packed);

    //
    // DEBUG
    //
#if 0
  {
    uint debug_base = 0;

    if (gl_SubgroupInvocationID == 0)
      debug_base = atomicAdd(bp_debug_count[0], SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE);

    debug_base = subgroupBroadcast(debug_base,0);

    bp_debug[debug_base + gl_SubgroupInvocationID] = rast_base;
  }
#endif

  //
  // FIXME(allanmac): the stores produced by this remaining code are
  // not coalesced.  It might not matter but, if it does, we can
  // probably do better and still retain portability.
  //
  // The strategy would to use local memory to accrue 5 subgroup-width
  // arrays of OR'd column bitmasks of path primitives.
  //
  // Then each row of bitmasks is loaded, a ballot is taken and the key
  // indices are stored either directly to global memory or back to
  // local for another pass.
  //
  {
    //
    // how many tags are there per lane?
    //
    SPN_FILLS_EXPAND_TAG_COUNT_INIT(tags_count);

#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  if (!SPN_PATH_HEAD_ENTIRELY_HEADER(SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE, I))                    \
    {                                                                                              \
      h##I = SPN_TAGGED_BLOCK_ID_GET_TAG(h##I);                                                    \
      SPN_FILLS_EXPAND_TAG_COUNT_ADD(tags_count, h##I);                                            \
    }

    SPN_FILLS_EXPAND_BLOCK_EXPAND();

    //
    // DEBUG -- dump the head with tag counts replaced
    //
#if 0
    {
      uint debug_base = 0;

      if (gl_SubgroupInvocationID == 0)
        debug_base = atomicAdd(bp_debug_count[0], SPN_BLOCK_POOL_BLOCK_DWORDS);

      debug_base = subgroupBroadcast(debug_base, 0);

#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  bp_debug[debug_base + I * SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE + gl_SubgroupInvocationID] = h##I;

      SPN_FILLS_EXPAND_BLOCK_EXPAND();
    }
#endif

    //
    // DEBUG -- dump the tags_count
    //
#if 0
    {
      uint debug_base = 0;

      if (gl_SubgroupInvocationID == 0)
        debug_base = atomicAdd(bp_debug_count[0], SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE);

      debug_base = subgroupBroadcast(debug_base, 0);

      bp_debug[debug_base + gl_SubgroupInvocationID] = tags_count;
    }
#endif

    //
    // we now have tag counts for this node...
    //
    // write out rasterize cmds for each tag type
    //
    // FIXME(allanmac): expand the outer tag loop so we can use broadcasts and not shuffles
    //
    for (uint ii = 0; ii < SPN_BLOCK_ID_TAG_PATH_COUNT; ii++)
      {
        const uint tag_count = SPN_FILLS_EXPAND_TAG_COUNT_GET(tags_count, ii);

        if (subgroupAny(tag_count > 0))
          {
            const uint tag_inc = subgroupInclusiveAdd(tag_count);
            const uint tag_exc = tag_inc - tag_count;

            SPN_SUBGROUP_UNIFORM const uint tag_red =
              subgroupBroadcast(tag_inc, SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE - 1);

            uint tag_rast_idx = subgroupShuffle(rast_base, ii) + tag_exc;

            if (gl_SubgroupInvocationID == ii)
              {
                rast_base += tag_red;
              }

#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  if (!SPN_PATH_HEAD_ENTIRELY_HEADER(SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE, I))                    \
    {                                                                                              \
      if (h##I == ii)                                                                              \
        {                                                                                          \
          SPN_CMD_RASTERIZE_SET_NODE_DWORD(cmd, SPN_CMD_RASTERIZE_I_TO_NODE_DWORD(I));             \
          rast_cmds[tag_rast_idx++] = cmd;                                                         \
        }                                                                                          \
    }

            SPN_FILLS_EXPAND_BLOCK_EXPAND();
          }
      }
  }

  //
  // are we done?
  //
  if (count_nodes == 0)
    return;

  //
  // get the "next" id - id of next block is in last lane
  //
  node_id = subgroupBroadcast(SPN_GLSL_CONCAT(h, SPN_FILLS_EXPAND_BLOCK_EXPAND_I_LAST),
                              SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE - 1);

  SPN_CMD_RASTERIZE_SET_NODE_ID(cmd, node_id);

  //
  // process next node
  //
  while (true)
    {
      //
      // load entire node
      //
      const uint n_idx = node_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS + gl_SubgroupInvocationID;

#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  uint n##I = bp_blocks[n_idx + I * SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE];

      SPN_FILLS_EXPAND_BLOCK_EXPAND();

      //
      // how many tags are there per lane?
      //
      SPN_FILLS_EXPAND_TAG_COUNT_INIT(tags_count);

#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  {                                                                                                \
    n##I = SPN_TAGGED_BLOCK_ID_GET_TAG(n##I);                                                      \
    SPN_FILLS_EXPAND_TAG_COUNT_ADD(tags_count, n##I);                                              \
  }

      SPN_FILLS_EXPAND_BLOCK_EXPAND();

      //
      // we now have tag counts for this node...
      //
      // write out rasterize cmds for each tag type
      //
      // FIXME(allanmac): expand the outer tag loop so we can use broadcasts and not shuffles
      //
      for (uint ii = 0; ii < SPN_BLOCK_ID_TAG_PATH_COUNT; ii++)
        {
          const uint tag_count = SPN_FILLS_EXPAND_TAG_COUNT_GET(tags_count, ii);

          if (subgroupAny(tag_count > 0))
            {
              const uint tag_inc = subgroupInclusiveAdd(tag_count);
              const uint tag_exc = tag_inc - tag_count;

              SPN_SUBGROUP_UNIFORM const uint tag_red =
                subgroupBroadcast(tag_inc, SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE - 1);

              uint tag_rast_idx = subgroupShuffle(rast_base, ii) + tag_exc;

              if (gl_SubgroupInvocationID == ii)
                {
                  rast_base += tag_red;
                }

#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  if (n##I == ii)                                                                                  \
    {                                                                                              \
      SPN_CMD_RASTERIZE_SET_NODE_DWORD(cmd, SPN_CMD_RASTERIZE_I_TO_NODE_DWORD(I));                 \
      rast_cmds[tag_rast_idx++] = cmd;                                                             \
    }

              SPN_FILLS_EXPAND_BLOCK_EXPAND();
            }
        }

      //
      // are we done?
      //
      if (--count_nodes == 0)
        return;

      //
      // get the "next" id - id of next block is in last lane
      //
      node_id = subgroupBroadcast(SPN_GLSL_CONCAT(h, SPN_FILLS_EXPAND_BLOCK_EXPAND_I_LAST),
                                  SPN_KERNEL_FILLS_EXPAND_SUBGROUP_SIZE - 1);

      SPN_CMD_RASTERIZE_SET_NODE_ID(cmd, node_id);
    }
}

//
//
//
